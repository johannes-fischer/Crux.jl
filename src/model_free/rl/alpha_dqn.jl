struct AlphaVec
    alpha::Vector{Float32}
    action::Any
end

# adds probabilities of terminals in b to bâ€² and normalizes bâ€²
function belief_norm(pomdp, b, bâ€², terminals, not_terminals)
    if sum(bâ€²[not_terminals]) != 0.
        if !isempty(terminals)
            bâ€²[not_terminals] = bâ€²[not_terminals] / (sum(bâ€²[not_terminals]) / (1. - sum(b[terminals]) - sum(bâ€²[terminals])))
            bâ€²[terminals] += b[terminals]
        else
            bâ€²[not_terminals] /= sum(bâ€²[not_terminals])
        end
    else
        bâ€²[terminals] += b[terminals]
        bâ€²[terminals] /= sum(bâ€²[terminals])
    end
    return bâ€²
end

function _argmax(f, X)
    return X[argmax(map(f, X))]
end

function alphaDVN_target_single_bao(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
    mdp = Ï€.mdp
    alphas = map(eachcol(ğ’Ÿ[:s])) do s
        @assert length(s) == 2
        b = convert_s(DiscreteBelief, collect(s), mdp)
        pomdp = mdp.pomdp

        S = POMDPTools.ordered_states(pomdp)
        A = POMDPTools.ordered_actions(pomdp)
        O = POMDPTools.ordered_observations(pomdp)
        r = StateActionReward(pomdp)

        Î“a = Vector{Vector{Float32}}(undef, length(A))

        not_terminals = [stateindex(pomdp, s) for s in S if !isterminal(pomdp, s)]
        terminals = [stateindex(pomdp, s) for s in S if isterminal(pomdp, s)]
        for a in A
            Î“ao = Vector{Vector{Float32}}(undef, length(O))
            trans_probs = dropdims(sum([pdf(transition(pomdp, S[is], a), sp) * b.b[is] for sp in S, is in not_terminals], dims=2), dims=2)
            if !isempty(terminals) trans_probs[terminals] .+= b.b[terminals] end

            for o in O
                # update beliefs
                obs_probs = pdf.(map(sp -> observation(pomdp, a, sp), S), [o])
                bâ€² = obs_probs .* trans_probs

                if sum(bâ€²) > 0.
                    bâ€² = DiscreteBelief(pomdp, b.state_list, belief_norm(pomdp, b.b, bâ€², terminals, not_terminals))
                else
                    bâ€² = DiscreteBelief(pomdp, b.state_list, zeros(length(S)))
                end

                # extract optimal alpha vector at resulting belief
                # Î“ao[obsindex(pomdp, o)] = _argmax(Î± -> dot(Î±,bâ€².b), Î“)
                bao_vec = convert_s(Vector, bâ€², mdp)
                Î“ao[obsindex(pomdp, o)] = value(Ï€, bao_vec)
            end

            # construct new alpha vectors
            Î“a[actionindex(pomdp, a)] = [r(s, a) + (!isterminal(pomdp, s) ? (Î³ * sum(pdf(transition(pomdp, s, a), sp) * pdf(observation(pomdp, s, a, sp), o) * Î“ao[i][j]
                                            for (j, sp) in enumerate(S), (i, o) in enumerate(O))) : 0.)
                                            for s in S]
        end

        # find the optimal alpha vector
        idx = argmax(map(Î±a -> Î±a â‹… b.b, Î“a))
        alphavec = AlphaVec(Î“a[idx], A[idx])
        return alphavec.alpha
    end
    reduce(hcat, alphas)
end

function alphaDVN_target_all_bao(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
    mdp = Ï€.mdp
    alphas = map(eachcol(ğ’Ÿ[:s])) do s
        @assert length(s) == 2
        b = convert_s(DiscreteBelief, collect(s), mdp)
        pomdp = mdp.pomdp

        S = ordered_states(pomdp)
        A = ordered_actions(pomdp)
        O = ordered_observations(pomdp)
        r = StateActionReward(pomdp)


        not_terminals = [stateindex(pomdp, s) for s in S if !isterminal(pomdp, s)]
        terminals = [stateindex(pomdp, s) for s in S if isterminal(pomdp, s)]

        Î“net = Vector{Vector{Float64}}(undef, length(A) * length(O))
        i = 1

        for a in A
            trans_probs = dropdims(sum([pdf(transition(pomdp, S[is], a), sp) * b.b[is] for sp in S, is in not_terminals], dims=2), dims=2)
            if !isempty(terminals) trans_probs[terminals] .+= b.b[terminals] end

            for o in O
                # update beliefs
                obs_probs = pdf.(map(sp -> observation(pomdp, a, sp), S), [o])
                bâ€² = obs_probs .* trans_probs

                if sum(bâ€²) > 0.
                    bâ€² = DiscreteBelief(pomdp, b.state_list, belief_norm(pomdp, b.b, bâ€², terminals, not_terminals))
                else
                    bâ€² = DiscreteBelief(pomdp, b.state_list, zeros(length(S)))
                end

                # extract optimal alpha vector at resulting belief
                # Î“ao[obsindex(pomdp, o)] = _argmax(Î± -> dot(Î±,bâ€².b), Î“)
                bao_vec = convert_s(Vector, bâ€², mdp)
                Î“net[i] = value(Ï€, bao_vec)
                i += 1
            end
        end

        Î“a = Vector{Vector{Float32}}(undef, length(A))
        for a in A
            Î“ao = Vector{Vector{Float32}}(undef, length(O))
            trans_probs = dropdims(sum([pdf(transition(pomdp, S[is], a), sp) * b.b[is] for sp in S, is in not_terminals], dims=2), dims=2)
            if !isempty(terminals) trans_probs[terminals] .+= b.b[terminals] end

            for o in O
                # update beliefs
                obs_probs = pdf.(map(sp -> observation(pomdp, a, sp), S), [o])
                bâ€² = obs_probs .* trans_probs

                if sum(bâ€²) > 0.
                    bâ€² = DiscreteBelief(pomdp, b.state_list, belief_norm(pomdp, b.b, bâ€², terminals, not_terminals))
                else
                    bâ€² = DiscreteBelief(pomdp, b.state_list, zeros(length(S)))
                end

                # extract optimal alpha vector at resulting belief
                Î“ao[obsindex(pomdp, o)] = _argmax(Î± -> dot(Î±,bâ€².b), Î“net)
            end

            # construct new alpha vectors
            Î“a[actionindex(pomdp, a)] = [r(s, a) + (!isterminal(pomdp, s) ? (Î³ * sum(pdf(transition(pomdp, s, a), sp) * pdf(observation(pomdp, s, a, sp), o) * Î“ao[i][j]
                                            for (j, sp) in enumerate(S), (i, o) in enumerate(O))) : 0.)
                                            for s in S]
        end

        # find the optimal alpha vector
        idx = argmax(map(Î±a -> Î±a â‹… b.b, Î“a))
        alphavec = AlphaVec(Î“a[idx], A[idx])
        return alphavec.alpha
    end
    reduce(hcat, alphas)
end

function sampledAlphaDVN(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
    mdp = Ï€.mdp
    alphas = map(eachcol(ğ’Ÿ[:s])) do s
        @assert length(s) == 2
        B = state_type(mdp)
        b = convert_s(B, collect(s), mdp)
        pomdp = mdp.pomdp

        S = POMDPTools.ordered_states(pomdp)
        A = POMDPTools.ordered_actions(pomdp)
        O = POMDPTools.ordered_observations(pomdp)
        r = StateActionReward(pomdp)

        Î“a = Vector{Vector{Float32}}(undef, length(A))

        not_terminals = [stateindex(pomdp, s) for s in S if !isterminal(pomdp, s)]
        terminals = [stateindex(pomdp, s) for s in S if isterminal(pomdp, s)]
        for a in A
            Î“ao = Vector{Vector{Float32}}(undef, length(O))
            trans_probs = dropdims(sum([pdf(transition(pomdp, S[is], a), sp) * b.b[is] for sp in S, is in not_terminals], dims=2), dims=2)
            if !isempty(terminals) trans_probs[terminals] .+= b.b[terminals] end

            for o in O
                # update beliefs
                obs_probs = pdf.(map(sp -> observation(pomdp, a, sp), S), [o])
                bâ€² = obs_probs .* trans_probs

                if sum(bâ€²) > 0.
                    bâ€² = B(pomdp, belief_norm(pomdp, b.b, bâ€², terminals, not_terminals))
                else
                    bâ€² = B(pomdp, zeros(length(S)))
                end

                # extract optimal alpha vector at resulting belief
                # Î“ao[obsindex(pomdp, o)] = _argmax(Î± -> dot(Î±,bâ€².b), Î“)
                bao_vec = convert_s(Vector, bâ€², mdp)
                Î“ao[obsindex(pomdp, o)] = value(Ï€, bao_vec)
            end

            # construct new alpha vectors
            Î“a[actionindex(pomdp, a)] = [r(s, a) + (!isterminal(pomdp, s) ? (Î³ * sum(pdf(transition(pomdp, s, a), sp) * pdf(observation(pomdp, s, a, sp), o) * Î“ao[i][j]
                                            for (j, sp) in enumerate(S), (i, o) in enumerate(O))) : 0.)
                                            for s in S]
        end

        # find the optimal alpha vector
        idx = argmax(map(Î±a -> Î±a â‹… b.b, Î“a))
        alphavec = AlphaVec(Î“a[idx], A[idx])
        return alphavec.alpha
    end
    reduce(hcat, alphas)
end

function alphaDQN_target(Ï€, ğ’«, ğ’Ÿ, Î³::Float32; kwargs...)
    mdp = Ï€.mdp
    pomdp = mdp.pomdp
    S = POMDPTools.ordered_states(pomdp)
    A = POMDPTools.ordered_actions(pomdp)
    O = POMDPTools.ordered_observations(pomdp)
    r = StateActionReward(pomdp)
    ns = length(S)
    na = length(A)

    alphas = map(zip(eachcol(ğ’Ÿ[:s]), eachcol(ğ’Ÿ[:a]))) do (s, a_oh)
        b = convert_s(DiscreteBelief, collect(s), mdp)

        @assert length(s) == length(states(pomdp))
        @assert length(a_oh) == length(actions(pomdp))

        not_terminals = [stateindex(pomdp, s) for s in S if !isterminal(pomdp, s)]
        terminals = [stateindex(pomdp, s) for s in S if isterminal(pomdp, s)]

        a = A[argmax(a_oh)]

        Î“ao = Vector{Vector{Float32}}(undef, length(O))
        trans_probs = dropdims(sum([pdf(transition(pomdp, S[is], a), sp) * b.b[is] for sp in S, is in not_terminals], dims=2), dims=2)
        if !isempty(terminals) trans_probs[terminals] .+= b.b[terminals] end

        for o in O
            # update beliefs
            obs_probs = pdf.(map(sp -> observation(pomdp, a, sp), S), [o])
            bâ€² = obs_probs .* trans_probs

            if sum(bâ€²) > 0.
                bâ€² = DiscreteBelief(pomdp, b.state_list, belief_norm(pomdp, b.b, bâ€², terminals, not_terminals))
            else
                bâ€² = DiscreteBelief(pomdp, b.state_list, zeros(length(S)))
            end

            # extract optimal alpha vector at resulting belief
            # Î“ao[obsindex(pomdp, o)] = _argmax(Î± -> dot(Î±,bâ€².b), Î“)
            bao_vec = convert_s(Vector, bâ€², mdp)
            Î±Q = reshape(value(Ï€.Ï€, bao_vec), (ns, na))
            idx = argmax(map(Î±a -> Î±a â‹… bao_vec, eachcol(Î±Q)))
            # alphavec = AlphaVec(Î“a[idx], A[idx])
            # idx = argmax(reshape(bao_vec * Î±Q, na))
            Î“ao[obsindex(pomdp, o)] = Î±Q[:, idx]
        end

        # construct new alpha vectors
        Î± = Float32[r(s, a) + (!isterminal(pomdp, s) ? (Î³ * sum(pdf(transition(pomdp, s, a), sp) * pdf(observation(pomdp, s, a, sp), o) * Î“ao[i][j]
                                        for (j, sp) in enumerate(S), (i, o) in enumerate(O))) : 0.)
                                        for s in S]

        # find the optimal alpha vector
        # idx = argmax(map(Î±a -> Î±a â‹… b.b, Î“a))
        # alphavec = AlphaVec(Î“a[idx], A[idx])
        return Î±
    end
    res = reduce(hcat, alphas)
    reshaped = reshape(res, (length(S), 1, length(alphas)))

    reshaped
end

function AlphaDQN(;Ï€::Crux.Policy,
              N::Int,
              Î”N=4,
              Ï€_explore=ÏµGreedyPolicy(LinearDecaySchedule(1., 0.1, floor(Int, N/2)), Ï€.outputs),
              c_opt::NamedTuple=(;),
              log::NamedTuple=(;),
              c_loss=td_Qloss(),
              target_fn=alphaDQN_target,
              prefix="",
              kwargs...)

     OffPolicySolver(;agent=PolicyParams(Ï€=Ï€, Ï€_explore=Ï€_explore, Ï€â»=deepcopy(Ï€)),
                      log=LoggerParams(;dir="log/dqn", log...),
                      N=N,
                      Î”N=Î”N,
                      c_opt = TrainingParams(;loss=c_loss, name=string(prefix, "critic_"), epochs=Î”N, c_opt...),
                      target_fn=target_fn,
                      kwargs...)
end


